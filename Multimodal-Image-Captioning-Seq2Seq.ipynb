{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, re, random, math, pickle\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:27:55.384604Z","iopub.execute_input":"2026-02-10T17:27:55.385373Z","iopub.status.idle":"2026-02-10T17:27:55.795171Z","shell.execute_reply.started":"2026-02-10T17:27:55.385342Z","shell.execute_reply":"2026-02-10T17:27:55.794631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, pickle, torch, torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\ndef find_image_dir():\n    # Common Kaggle root\n    base_input = '/kaggle/input'\n\n    # Walk through the input directory to find where the images actually are\n    for root, dirs, files in os.walk(base_input):\n        # Look for the folder containing a high volume of jpg files\n        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n            return root\n\n    return None\n\n\nIMAGE_DIR = find_image_dir()\nOUTPUT_FILE = 'flickr30k_features.pkl'\n\nif IMAGE_DIR:\n    print(f\" Found images at: {IMAGE_DIR}\")\nelse:\n    raise FileNotFoundError(\n        \"Could not find the Flickr30k image directory. Please ensure the dataset \"\n        \"is added to the notebook.\"\n    )\n\n\n# --- THE DATASET CLASS ---\nclass FlickrDataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.img_names = [\n            f for f in os.listdir(img_dir)\n            if f.endswith(('.jpg', '.jpeg'))\n        ]\n        self.transform = transform\n        self.img_dir = img_dir\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, idx):\n        name = self.img_names[idx]\n        img_path = os.path.join(self.img_dir, name)\n        img = Image.open(img_path).convert('RGB')\n        return self.transform(img), name\n\n\n# --- REMAINDER OF THE PIPELINE (AS BEFORE) ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nmodel = nn.Sequential(*list(model.children())[:-1])  # Feature vector only\nmodel = nn.DataParallel(model).to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        (0.485, 0.456, 0.406),\n        (0.229, 0.224, 0.225)\n    )\n])\n\ndataset = FlickrDataset(IMAGE_DIR, transform)\nloader = DataLoader(dataset, batch_size=128, num_workers=4)\n\nfeatures_dict = {}\n\nwith torch.no_grad():\n    for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n        feats = model(imgs.to(device)).view(imgs.size(0), -1)\n        for i, name in enumerate(names):\n            features_dict[name] = feats[i].cpu().numpy()\n\nwith open(OUTPUT_FILE, 'wb') as f:\n    pickle.dump(features_dict, f)\n\nprint(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:23:25.265108Z","iopub.execute_input":"2026-02-10T17:23:25.265822Z","iopub.status.idle":"2026-02-10T17:25:46.532382Z","shell.execute_reply.started":"2026-02-10T17:23:25.265788Z","shell.execute_reply":"2026-02-10T17:25:46.531561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Find Dataset Files**","metadata":{}},{"cell_type":"code","source":"def find_file_in_kaggle_input(filename):\n    base_input = \"/kaggle/input\"\n    for root, dirs, files in os.walk(base_input):\n        if filename in files:\n            return os.path.join(root, filename)\n    return None\n\ndef find_image_dir():\n    base_input = \"/kaggle/input\"\n    best_dir = None\n    best_count = 0\n    for root, dirs, files in os.walk(base_input):\n        jpgs = [f for f in files if f.lower().endswith((\".jpg\", \".jpeg\"))]\n        if len(jpgs) > best_count:\n            best_count = len(jpgs)\n            best_dir = root\n    return best_dir\n\nCAPTIONS_PATH = find_file_in_kaggle_input(\"captions.txt\")\nIMAGE_DIR = find_image_dir()\n\nprint(\"CAPTIONS_PATH:\", CAPTIONS_PATH)\nprint(\"IMAGE_DIR:\", IMAGE_DIR)\n\nassert CAPTIONS_PATH is not None, \"captions.txt not found. Make sure Flickr30k dataset is added.\"\nassert IMAGE_DIR is not None, \"Image directory not found. Make sure Flickr30k dataset is added.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:26:25.196603Z","iopub.execute_input":"2026-02-10T17:26:25.197307Z","iopub.status.idle":"2026-02-10T17:26:56.903086Z","shell.execute_reply.started":"2026-02-10T17:26:25.197268Z","shell.execute_reply":"2026-02-10T17:26:56.902339Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load Caption & Clean Text**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(CAPTIONS_PATH)\nprint(df.head())\nprint(df.columns)\nprint(\"Rows:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:28:03.633295Z","iopub.execute_input":"2026-02-10T17:28:03.633698Z","iopub.status.idle":"2026-02-10T17:28:03.960053Z","shell.execute_reply.started":"2026-02-10T17:28:03.633674Z","shell.execute_reply":"2026-02-10T17:28:03.959304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_caption(text: str) -> str:\n    text = str(text).lower()\n    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# Try to detect correct column names\n# Common possibilities: ['image', 'caption'] or ['image_name', 'comment']\nimg_col = None\ncap_col = None\n\nfor c in df.columns:\n    if \"image\" in c.lower():\n        img_col = c\n    if \"caption\" in c.lower() or \"comment\" in c.lower() or \"text\" in c.lower():\n        cap_col = c\n\nassert img_col is not None and cap_col is not None, f\"Could not detect image/caption columns from: {df.columns}\"\n\ndf = df[[img_col, cap_col]].rename(columns={img_col: \"image\", cap_col: \"caption\"})\ndf[\"caption\"] = df[\"caption\"].apply(clean_caption)\ndf[\"caption\"] = df[\"caption\"].apply(lambda x: f\"<start> {x} <end>\")\n\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:28:07.924899Z","iopub.execute_input":"2026-02-10T17:28:07.925561Z","iopub.status.idle":"2026-02-10T17:28:09.002152Z","shell.execute_reply.started":"2026-02-10T17:28:07.925530Z","shell.execute_reply":"2026-02-10T17:28:09.001481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load Cached Features**","metadata":{}},{"cell_type":"code","source":"FEATURES_PATH = \"flickr30k_features.pkl\"\nassert os.path.exists(FEATURES_PATH), \"Run feature extraction cell first to create flickr30k_features.pkl\"\n\nwith open(FEATURES_PATH, \"rb\") as f:\n    features_dict = pickle.load(f)\n\nprint(\"Loaded features for images:\", len(features_dict))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:28:19.129888Z","iopub.execute_input":"2026-02-10T17:28:19.130570Z","iopub.status.idle":"2026-02-10T17:28:19.477665Z","shell.execute_reply.started":"2026-02-10T17:28:19.130543Z","shell.execute_reply":"2026-02-10T17:28:19.476914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df[\"image\"].isin(features_dict.keys())].reset_index(drop=True)\nprint(\"After filtering:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:28:24.916178Z","iopub.execute_input":"2026-02-10T17:28:24.916839Z","iopub.status.idle":"2026-02-10T17:28:24.970167Z","shell.execute_reply.started":"2026-02-10T17:28:24.916808Z","shell.execute_reply":"2026-02-10T17:28:24.969403Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Build Vocabulary**","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\nPAD_TOKEN = \"<pad>\"\nUNK_TOKEN = \"<unk>\"\n\ndef build_vocab(captions, min_freq=3):\n    counter = Counter()\n    for cap in captions:\n        counter.update(cap.split())\n    \n    vocab = [PAD_TOKEN, UNK_TOKEN, \"<start>\", \"<end>\"]\n    for word, freq in counter.items():\n        if freq >= min_freq and word not in vocab:\n            vocab.append(word)\n    return vocab\n\nvocab = build_vocab(df[\"caption\"].tolist(), min_freq=3)\nword2idx = {w:i for i,w in enumerate(vocab)}\nidx2word = {i:w for w,i in word2idx.items()}\n\npad_id = word2idx[PAD_TOKEN]\nunk_id = word2idx[UNK_TOKEN]\nstart_id = word2idx[\"<start>\"]\nend_id = word2idx[\"<end>\"]\n\nprint(\"Vocab size:\", len(vocab))\nprint(\"pad_id:\", pad_id, \"start_id:\", start_id, \"end_id:\", end_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:29:00.721033Z","iopub.execute_input":"2026-02-10T17:29:00.721353Z","iopub.status.idle":"2026-02-10T17:29:01.608427Z","shell.execute_reply.started":"2026-02-10T17:29:00.721327Z","shell.execute_reply":"2026-02-10T17:29:01.607754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Convert captions to ids + pad/truncate**","metadata":{}},{"cell_type":"code","source":"def encode_caption(cap, word2idx):\n    return [word2idx.get(w, word2idx[UNK_TOKEN]) for w in cap.split()]\n\nencoded = [encode_caption(c, word2idx) for c in df[\"caption\"].tolist()]\nlengths = [len(x) for x in encoded]\nmax_len = int(np.percentile(lengths, 95))  # simple robust max\nmax_len = max(10, min(max_len, 40))        # keep it reasonable\nprint(\"Max_len used:\", max_len)\n\ndef pad_or_trunc(seq, max_len, pad_id):\n    if len(seq) < max_len:\n        return seq + [pad_id]*(max_len-len(seq))\n    return seq[:max_len]\n\ndf[\"cap_ids\"] = [pad_or_trunc(x, max_len, pad_id) for x in encoded]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:29:49.300117Z","iopub.execute_input":"2026-02-10T17:29:49.300859Z","iopub.status.idle":"2026-02-10T17:29:50.256974Z","shell.execute_reply.started":"2026-02-10T17:29:49.300827Z","shell.execute_reply":"2026-02-10T17:29:50.256133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Train/Val/Test Split**","metadata":{}},{"cell_type":"code","source":"all_images = df[\"image\"].unique().tolist()\nrandom.seed(42)\nrandom.shuffle(all_images)\n\nn = len(all_images)\ntrain_imgs = set(all_images[:int(0.8*n)])\nval_imgs   = set(all_images[int(0.8*n):int(0.9*n)])\ntest_imgs  = set(all_images[int(0.9*n):])\n\ntrain_df = df[df[\"image\"].isin(train_imgs)].reset_index(drop=True)\nval_df   = df[df[\"image\"].isin(val_imgs)].reset_index(drop=True)\ntest_df  = df[df[\"image\"].isin(test_imgs)].reset_index(drop=True)\n\nprint(len(train_df), len(val_df), len(test_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:30:18.807318Z","iopub.execute_input":"2026-02-10T17:30:18.807865Z","iopub.status.idle":"2026-02-10T17:30:18.896260Z","shell.execute_reply.started":"2026-02-10T17:30:18.807837Z","shell.execute_reply":"2026-02-10T17:30:18.895614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataset + Dataloader**","metadata":{}},{"cell_type":"code","source":"class CaptionDataset(Dataset):\n    def __init__(self, data_df, features_dict):\n        self.df = data_df\n        self.features = features_dict\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_name = row[\"image\"]\n        feat = torch.tensor(self.features[img_name], dtype=torch.float32)  # [2048]\n        \n        cap = torch.tensor(row[\"cap_ids\"], dtype=torch.long)              # [max_len]\n        x = cap[:-1]  # input\n        y = cap[1:]   # target\n        return feat, x, y, img_name\n\nbatch_size = 128\ntrain_loader = DataLoader(CaptionDataset(train_df, features_dict), batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader   = DataLoader(CaptionDataset(val_df, features_dict), batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(CaptionDataset(test_df, features_dict), batch_size=64, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:30:58.281032Z","iopub.execute_input":"2026-02-10T17:30:58.281671Z","iopub.status.idle":"2026-02-10T17:30:58.288133Z","shell.execute_reply.started":"2026-02-10T17:30:58.281645Z","shell.execute_reply":"2026-02-10T17:30:58.287502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Seq2Seq Model (Encoder + Decoder)**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, in_dim=2048, hidden_size=512):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, hidden_size)\n        self.relu = nn.ReLU()\n        \n    def forward(self, feat):\n        # feat: [B, 2048]\n        h = self.relu(self.fc(feat))  # [B, hidden]\n        return h\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hidden_size=512, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers, batch_first=True, dropout=0.0)\n        self.dropout = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x_tokens, h0):\n        \"\"\"\n        x_tokens: [B, T] (teacher forcing input)\n        h0: [B, hidden]\n        \"\"\"\n        emb = self.dropout(self.embed(x_tokens))  # [B, T, E]\n        \n        # LSTM initial state expects [num_layers, B, hidden]\n        h0 = h0.unsqueeze(0)                 # [1, B, hidden]\n        c0 = torch.zeros_like(h0)            # [1, B, hidden]\n        \n        out, _ = self.lstm(emb, (h0, c0))    # [B, T, hidden]\n        logits = self.fc_out(out)            # [B, T, vocab]\n        return logits\n\nclass Img2Caption(nn.Module):\n    def __init__(self, vocab_size, in_dim=2048, hidden_size=512, embed_dim=256):\n        super().__init__()\n        self.encoder = Encoder(in_dim=in_dim, hidden_size=hidden_size)\n        self.decoder = Decoder(vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size)\n        \n    def forward(self, feat, x_tokens):\n        h0 = self.encoder(feat)\n        logits = self.decoder(x_tokens, h0)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:32:11.591523Z","iopub.execute_input":"2026-02-10T17:32:11.592248Z","iopub.status.idle":"2026-02-10T17:32:11.601016Z","shell.execute_reply.started":"2026-02-10T17:32:11.592205Z","shell.execute_reply":"2026-02-10T17:32:11.600025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training Setup**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Img2Caption(vocab_size=len(vocab), hidden_size=512, embed_dim=256).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_id)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:32:28.344497Z","iopub.execute_input":"2026-02-10T17:32:28.344850Z","iopub.status.idle":"2026-02-10T17:32:28.463130Z","shell.execute_reply.started":"2026-02-10T17:32:28.344817Z","shell.execute_reply":"2026-02-10T17:32:28.462380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training + Validation Loop**","metadata":{}},{"cell_type":"code","source":"def run_epoch(model, loader, train=True):\n    if train:\n        model.train()\n    else:\n        model.eval()\n        \n    total_loss = 0.0\n    total_tokens = 0\n    \n    for feat, x, y, _ in tqdm(loader, leave=False):\n        feat = feat.to(device)\n        x = x.to(device)        # [B, T]\n        y = y.to(device)        # [B, T]\n        \n        if train:\n            optimizer.zero_grad()\n        \n        with torch.set_grad_enabled(train):\n            logits = model(feat, x)  # [B, T, V]\n            loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n            \n            if train:\n                loss.backward()\n                nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n                optimizer.step()\n        \n        # count non-pad tokens for a fair avg (optional)\n        non_pad = (y != pad_id).sum().item()\n        total_loss += loss.item() * non_pad\n        total_tokens += non_pad\n    \n    return total_loss / max(1, total_tokens)\n\nepochs = 50  \ntrain_losses, val_losses = [], []\n\nfor ep in range(1, epochs+1):\n    tr = run_epoch(model, train_loader, train=True)\n    va = run_epoch(model, val_loader, train=False)\n    train_losses.append(tr)\n    val_losses.append(va)\n    print(f\"Epoch {ep}/{epochs} | train loss: {tr:.4f} | val loss: {va:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:33:34.807053Z","iopub.execute_input":"2026-02-10T17:33:34.807380Z","iopub.status.idle":"2026-02-10T18:14:14.374492Z","shell.execute_reply.started":"2026-02-10T17:33:34.807355Z","shell.execute_reply":"2026-02-10T18:14:14.373565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plot Loss Curves**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,4))\nplt.plot(train_losses, label=\"train\")\nplt.plot(val_losses, label=\"val\")\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:23.023809Z","iopub.execute_input":"2026-02-10T18:24:23.024147Z","iopub.status.idle":"2026-02-10T18:24:23.180011Z","shell.execute_reply.started":"2026-02-10T18:24:23.024116Z","shell.execute_reply":"2026-02-10T18:24:23.179437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Token Decode**","metadata":{}},{"cell_type":"code","source":"def decode_tokens(token_ids):\n    words = []\n    for tid in token_ids:\n        w = idx2word.get(int(tid), UNK_TOKEN)\n        if w == \"<end>\":\n            break\n        if w not in [\"<start>\", \"<pad>\"]:\n            words.append(w)\n    return \" \".join(words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:31.401850Z","iopub.execute_input":"2026-02-10T18:24:31.402400Z","iopub.status.idle":"2026-02-10T18:24:31.406623Z","shell.execute_reply.started":"2026-02-10T18:24:31.402371Z","shell.execute_reply":"2026-02-10T18:24:31.405826Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Greedy Search**","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef greedy_caption(model, feat_vec, max_len=30):\n    model.eval()\n    feat = torch.tensor(feat_vec, dtype=torch.float32).unsqueeze(0).to(device)  # [1,2048]\n    h0 = model.encoder(feat)  # [1,hidden]\n    \n    cur = torch.tensor([[start_id]], dtype=torch.long).to(device)  # [1,1]\n    out_tokens = []\n    \n    # we feed one token at a time\n    h = h0.unsqueeze(0)  # [1,1,hidden] for LSTM hidden\n    c = torch.zeros_like(h)\n    \n    for _ in range(max_len):\n        emb = model.decoder.embed(cur)  # [1,1,E]\n        lstm_out, (h, c) = model.decoder.lstm(emb, (h, c))  # [1,1,hidden]\n        logits = model.decoder.fc_out(lstm_out.squeeze(1))  # [1,V]\n        nxt = torch.argmax(logits, dim=-1).item()\n        \n        if nxt == end_id:\n            break\n        out_tokens.append(nxt)\n        cur = torch.tensor([[nxt]], dtype=torch.long).to(device)\n        \n    return decode_tokens(out_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:38.644717Z","iopub.execute_input":"2026-02-10T18:24:38.645251Z","iopub.status.idle":"2026-02-10T18:24:38.651897Z","shell.execute_reply.started":"2026-02-10T18:24:38.645198Z","shell.execute_reply":"2026-02-10T18:24:38.651083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Beam Search**","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef beam_caption(model, feat_vec, beam_size=3, max_len=30):\n    model.eval()\n    feat = torch.tensor(feat_vec, dtype=torch.float32).unsqueeze(0).to(device)\n    h0 = model.encoder(feat)  # [1,hidden]\n    \n    # each beam item: (tokens_list, logprob, h, c, last_token)\n    h = h0.unsqueeze(0)\n    c = torch.zeros_like(h)\n    \n    beams = [([], 0.0, h, c, start_id)]\n    \n    for _ in range(max_len):\n        new_beams = []\n        for tokens, score, h_i, c_i, last in beams:\n            if last == end_id:\n                new_beams.append((tokens, score, h_i, c_i, last))\n                continue\n            \n            cur = torch.tensor([[last]], dtype=torch.long).to(device)\n            emb = model.decoder.embed(cur)\n            lstm_out, (h_new, c_new) = model.decoder.lstm(emb, (h_i, c_i))\n            logits = model.decoder.fc_out(lstm_out.squeeze(1))  # [1,V]\n            log_probs = torch.log_softmax(logits, dim=-1).squeeze(0)  # [V]\n            \n            topk = torch.topk(log_probs, beam_size)\n            for lp, idx in zip(topk.values.tolist(), topk.indices.tolist()):\n                new_tokens = tokens + [idx]\n                new_score = score + lp\n                new_beams.append((new_tokens, new_score, h_new, c_new, idx))\n        \n        # keep best beams\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:beam_size]\n        \n        # if all ended\n        if all(b[4] == end_id for b in beams):\n            break\n    \n    best_tokens = beams[0][0]\n    # remove last token if it is <end>\n    if len(best_tokens) and best_tokens[-1] == end_id:\n        best_tokens = best_tokens[:-1]\n    return decode_tokens(best_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:47.756634Z","iopub.execute_input":"2026-02-10T18:24:47.757271Z","iopub.status.idle":"2026-02-10T18:24:47.765939Z","shell.execute_reply.started":"2026-02-10T18:24:47.757247Z","shell.execute_reply":"2026-02-10T18:24:47.765288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Display 5 Random Test Images**","metadata":{}},{"cell_type":"code","source":"def show_random_examples(k=5):\n    samples = test_df.sample(k, random_state=42)\n    for _, row in samples.iterrows():\n        img_name = row[\"image\"]\n        gt = row[\"caption\"]\n        feat = features_dict[img_name]\n        \n        pred_greedy = greedy_caption(model, feat, max_len=30)\n        pred_beam   = beam_caption(model, feat, beam_size=3, max_len=30)\n        \n        img_path = os.path.join(IMAGE_DIR, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        plt.figure(figsize=(5,5))\n        plt.imshow(img)\n        plt.axis(\"off\")\n        plt.title(img_name)\n        plt.show()\n        \n        print(\"GT:\", gt)\n        print(\"Greedy:\", pred_greedy)\n        print(\"Beam:\", pred_beam)\n        print(\"-\"*80)\n\nshow_random_examples(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:54.774886Z","iopub.execute_input":"2026-02-10T18:24:54.775518Z","iopub.status.idle":"2026-02-10T18:24:55.669442Z","shell.execute_reply.started":"2026-02-10T18:24:54.775491Z","shell.execute_reply":"2026-02-10T18:24:55.668674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:25:32.698054Z","iopub.execute_input":"2026-02-10T18:25:32.698722Z","iopub.status.idle":"2026-02-10T18:25:36.654467Z","shell.execute_reply.started":"2026-02-10T18:25:32.698693Z","shell.execute_reply":"2026-02-10T18:25:36.653453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**BLEU-4 Score**","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\nsmooth = SmoothingFunction().method1\n\ndef bleu4_score(model, data_df, num_samples=2000, use_beam=True):\n    df_eval = data_df.sample(min(num_samples, len(data_df)), random_state=123)\n    scores = []\n    for _, row in tqdm(df_eval.iterrows(), total=len(df_eval)):\n        img = row[\"image\"]\n        feat = features_dict[img]\n        \n        pred = beam_caption(model, feat, beam_size=3) if use_beam else greedy_caption(model, feat)\n        ref = row[\"caption\"].replace(\"<start>\",\"\").replace(\"<end>\",\"\").strip()\n        \n        ref_tokens = [ref.split()]\n        pred_tokens = pred.split()\n        if len(pred_tokens) == 0:\n            scores.append(0.0)\n        else:\n            scores.append(sentence_bleu(ref_tokens, pred_tokens, weights=(0.25,0.25,0.25,0.25), smoothing_function=smooth))\n    return float(np.mean(scores))\n\nbleu4 = bleu4_score(model, test_df, num_samples=2000, use_beam=True)\nprint(\"BLEU-4:\", bleu4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:25:42.966979Z","iopub.execute_input":"2026-02-10T18:25:42.967352Z","iopub.status.idle":"2026-02-10T18:26:24.458581Z","shell.execute_reply.started":"2026-02-10T18:25:42.967308Z","shell.execute_reply":"2026-02-10T18:26:24.457948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Token-level Precision / Recall / F1**","metadata":{}},{"cell_type":"code","source":"def prf1_token_level(model, data_df, num_samples=2000, use_beam=True):\n    df_eval = data_df.sample(min(num_samples, len(data_df)), random_state=1234)\n    \n    precisions, recalls, f1s = [], [], []\n    for _, row in tqdm(df_eval.iterrows(), total=len(df_eval)):\n        img = row[\"image\"]\n        feat = features_dict[img]\n        \n        pred = beam_caption(model, feat, beam_size=3) if use_beam else greedy_caption(model, feat)\n        ref = row[\"caption\"].replace(\"<start>\",\"\").replace(\"<end>\",\"\").strip()\n        \n        pred_tokens = [t for t in pred.split() if t]\n        ref_tokens  = [t for t in ref.split() if t]\n        \n        pred_set = set(pred_tokens)\n        ref_set  = set(ref_tokens)\n        \n        if len(pred_set) == 0:\n            precisions.append(0.0)\n            recalls.append(0.0)\n            f1s.append(0.0)\n            continue\n        \n        tp = len(pred_set & ref_set)\n        fp = len(pred_set - ref_set)\n        fn = len(ref_set - pred_set)\n        \n        p = tp / (tp + fp + 1e-9)\n        r = tp / (tp + fn + 1e-9)\n        f1 = 2*p*r / (p+r+1e-9)\n        \n        precisions.append(p)\n        recalls.append(r)\n        f1s.append(f1)\n    \n    return float(np.mean(precisions)), float(np.mean(recalls)), float(np.mean(f1s))\n\np, r, f1 = prf1_token_level(model, test_df, num_samples=2000, use_beam=True)\nprint(\"Token-level Precision:\", p)\nprint(\"Token-level Recall:\", r)\nprint(\"Token-level F1:\", f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:26:34.310880Z","iopub.execute_input":"2026-02-10T18:26:34.311353Z","iopub.status.idle":"2026-02-10T18:27:13.943420Z","shell.execute_reply.started":"2026-02-10T18:26:34.311322Z","shell.execute_reply":"2026-02-10T18:27:13.942715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**METEOR Score**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download(\"wordnet\", quiet=True)\nnltk.download(\"omw-1.4\", quiet=True)\n\nfrom nltk.translate.meteor_score import meteor_score\n\ndef meteor_eval(model, data_df, num_samples=2000, use_beam=True):\n    df_eval = data_df.sample(min(num_samples, len(data_df)), random_state=2026)\n\n    scores = []\n    for _, row in tqdm(df_eval.iterrows(), total=len(df_eval)):\n        img = row[\"image\"]\n        feat = features_dict[img]\n\n        pred = beam_caption(model, feat, beam_size=3) if use_beam else greedy_caption(model, feat)\n        ref  = row[\"caption\"].replace(\"<start>\",\"\").replace(\"<end>\",\"\").strip()\n\n        # meteor_score expects tokenized strings (list of references, and hypothesis)\n        # We pass tokens as lists\n        ref_tokens = ref.split()\n        pred_tokens = pred.split()\n\n        if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n            scores.append(0.0)\n        else:\n            scores.append(meteor_score([ref_tokens], pred_tokens))\n\n    return float(np.mean(scores))\n\nmeteor = meteor_eval(model, test_df, num_samples=2000, use_beam=True)\nprint(\"METEOR:\", meteor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:27:28.675116Z","iopub.execute_input":"2026-02-10T18:27:28.675452Z","iopub.status.idle":"2026-02-10T18:28:12.950492Z","shell.execute_reply.started":"2026-02-10T18:27:28.675424Z","shell.execute_reply":"2026-02-10T18:28:12.949772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ROUGE Scores  (ROUGE-1 / ROUGE-2 / ROUGE-L)**","metadata":{}},{"cell_type":"code","source":"!pip -q install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:28:17.300392Z","iopub.execute_input":"2026-02-10T18:28:17.301130Z","iopub.status.idle":"2026-02-10T18:28:20.376475Z","shell.execute_reply.started":"2026-02-10T18:28:17.301101Z","shell.execute_reply":"2026-02-10T18:28:20.375463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom rouge_score import rouge_scorer\n\ndef rouge_eval(model, data_df, num_samples=2000, use_beam=True):\n    df_eval = data_df.sample(min(num_samples, len(data_df)), random_state=2027)\n\n    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n\n    r1_p = []; r1_r = []; r1_f = []\n    r2_p = []; r2_r = []; r2_f = []\n    rl_p = []; rl_r = []; rl_f = []\n\n    for _, row in tqdm(df_eval.iterrows(), total=len(df_eval)):\n        img = row[\"image\"]\n        feat = features_dict[img]\n\n        pred = beam_caption(model, feat, beam_size=3) if use_beam else greedy_caption(model, feat)\n        ref  = row[\"caption\"].replace(\"<start>\",\"\").replace(\"<end>\",\"\").strip()\n\n        if len(pred.strip()) == 0 or len(ref.strip()) == 0:\n            # add zeros if empty\n            r1_p.append(0.0); r1_r.append(0.0); r1_f.append(0.0)\n            r2_p.append(0.0); r2_r.append(0.0); r2_f.append(0.0)\n            rl_p.append(0.0); rl_r.append(0.0); rl_f.append(0.0)\n            continue\n\n        scores = scorer.score(ref, pred)\n\n        r1_p.append(scores[\"rouge1\"].precision)\n        r1_r.append(scores[\"rouge1\"].recall)\n        r1_f.append(scores[\"rouge1\"].fmeasure)\n\n        r2_p.append(scores[\"rouge2\"].precision)\n        r2_r.append(scores[\"rouge2\"].recall)\n        r2_f.append(scores[\"rouge2\"].fmeasure)\n\n        rl_p.append(scores[\"rougeL\"].precision)\n        rl_r.append(scores[\"rougeL\"].recall)\n        rl_f.append(scores[\"rougeL\"].fmeasure)\n\n    results = {\n        \"ROUGE-1\": {\"P\": float(np.mean(r1_p)), \"R\": float(np.mean(r1_r)), \"F1\": float(np.mean(r1_f))},\n        \"ROUGE-2\": {\"P\": float(np.mean(r2_p)), \"R\": float(np.mean(r2_r)), \"F1\": float(np.mean(r2_f))},\n        \"ROUGE-L\": {\"P\": float(np.mean(rl_p)), \"R\": float(np.mean(rl_r)), \"F1\": float(np.mean(rl_f))},\n    }\n    return results\n\nrouge_results = rouge_eval(model, test_df, num_samples=2000, use_beam=True)\nrouge_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:28:23.981202Z","iopub.execute_input":"2026-02-10T18:28:23.982116Z","iopub.status.idle":"2026-02-10T18:29:04.793761Z","shell.execute_reply.started":"2026-02-10T18:28:23.982080Z","shell.execute_reply":"2026-02-10T18:29:04.793166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k, v in rouge_results.items():\n    print(f\"{k}: P={v['P']:.4f}, R={v['R']:.4f}, F1={v['F1']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:29:07.974503Z","iopub.execute_input":"2026-02-10T18:29:07.974785Z","iopub.status.idle":"2026-02-10T18:29:07.979538Z","shell.execute_reply.started":"2026-02-10T18:29:07.974763Z","shell.execute_reply":"2026-02-10T18:29:07.978658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Save Model**","metadata":{}},{"cell_type":"code","source":"torch.save({\n    \"model_state\": model.state_dict(),\n    \"word2idx\": word2idx,\n    \"idx2word\": idx2word,\n    \"max_len\": max_len,\n}, \"img_caption_seq2seq.pth\")\n\nprint(\"Saved: img_caption_seq2seq.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:29:14.200726Z","iopub.execute_input":"2026-02-10T18:29:14.201328Z","iopub.status.idle":"2026-02-10T18:29:14.281038Z","shell.execute_reply.started":"2026-02-10T18:29:14.201301Z","shell.execute_reply":"2026-02-10T18:29:14.280416Z"}},"outputs":[],"execution_count":null}]}